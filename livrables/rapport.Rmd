---
title: "Air quality project : Time Series Analysis"
author : Yousra Cherkaoui Tangi - Lou Peltier - Guilhem Bois
output: pdf_document
date : "`r format(Sys.time(), '%d %B, %Y')`"
---

\tableofcontents
\newpage
\section{Preliminary }

Our project consists of time series analysis using R programming language. Our data set contains the responses of a gas mutisensor device deployed on the field in an Italian city, in a significantly polluted area, at road level. The data set contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. Data were recorded from March 2004 to February 2005. 


```{r, echo=FALSE}
#Spécifier le chemin du fichier .csv 
fic = "/home/yousra/3A/cours/SeriesTemp/Air-Quality-Project/data/AirQualityUCI.csv"
data = read.csv(fic, header = TRUE, sep = ";", stringsAsFactors = FALSE)
```


```{r}
dim(data)
str(data)
```
| Attribute 	    	|Description                                                                                           |
|:-----------------:|:---------------------------------------------------------------------------------------------------- |    
| Date 		          | Date (DD/MM/YYYY)                                                                                    |
| Time 		          | Time (HH.MM.SS)                                                                                      |
| CO(GT) 		        | True hourly averaged concentration CO in mg/m^3 (reference analyzer)                                 |
| PT08.S1(CO)     	| PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)                          |
| NMHC(GT) 		      | True hourly averaged overall Non Metanic Hydro Carbons concentration in microg/m^3(reference analyzer |
| C6H6(GT) 		      | True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)                        |
| PT08.S2(NMHC)     | PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)                          |
| NOx(GT) 		      | True hourly averaged NOx concentration in ppb (reference analyzer)                                   |
| PT08.S3(NOx)    	| PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)                    |
| NO2(GT) 		      | True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)                            |
| PT08.S4(NO2)    	| PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)                    |
| PT08.S5(O3)     	| PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)                       |
| T 		            | Temperature in °C                                                                                   |
| RH 		            | Relative Humidity (%)                                                                                |
| AH 		            | AH Absolute Humidity                                                                                 |
 		

\section{I)-Cleaning and preprocessing}
\subsection{1) Data cleaning}
As we can see in the str(dim) output, our data contains missing values, some are directly reported as NA and others have been assigned to -200. It also contains a certain number of numeric variables that are characters which makes it not directly exploitable and some numbers have commas within them. For all these reasons, we had to clean our data set first so that we can exploit it afterwards.

```{r, echo=FALSE}
library(Amelia)
library(data.table)
missmap(data, main = "Missing values vs observed")
```


We remove X1 and X beacause they have no data within them, also we compute the necessary changes so that our data is exploitable afterwards.

```{r, echo=FALSE}
data$X = NULL
data$X.1 = NULL
#On supprime les lignes contenant des valeurs manquantes 
for(i in 1:15){
  data = subset(data, !is.na(data[,i]))
}

data$Date = as.Date(data$Date,format='%d/%m/%Y' )
data$CO.GT. = as.numeric(sub(",", ".", data$CO.GT.))
data$C6H6.GT. = as.numeric(sub(",", ".", data$C6H6.GT.))
data$T = as.numeric(sub(",", ".", data$T))
data$RH = as.numeric(sub(",", ".", data$RH))
data$AH = as.numeric(sub(",", ".", data$AH))
```

We noticed that NHMC contains a lot of -200 values, so we prefered to delet it along with all the lines that contain -200. 
```{r}
summary(data$NMHC.GT.)
```

```{r, echo=FALSE}
data$NMHC.GT.=NULL
for(j in 1:14){
 data = subset(data, data[,j]!=-200)
}

```

```{r}
str(data)
```

Now that our data is nearly clean, we chose to consider the daily concentration of the different gases our main focus of study by taking the average of the concentrations observed daily. 
```{r, echo=FALSE}
daily_data = copy(data)
daily_data$Time = NULL
daily_data = apply(daily_data[,2:13],2,tapply, daily_data$Date,mean )
daily_data = as.data.frame(daily_data)
setDT(daily_data, keep.rownames = "Date")
setDF(daily_data)
daily_data$Date = as.Date(daily_data$Date,format="%Y-%m-%d" )
```

We obtain 341 observations of 13 variables.

```{r}
str(daily_data)
```
```{r}
missmap(daily_data, main = "Missing values vs observed")
```

Since our data is all observed now, we can start plotting it to visualize what our data set looks like. 


```{r, echo=FALSE}
par(mfrow=c(2,2))
colors = c('red', 'blue', 'green','violet','orange','yellow','purple','brown')
v=c(11,12,13)
for(i in 2:13){
  j = sample(1:length(colors),1)
  if(i %in% v){
    plot(daily_data$Date, daily_data[,i],"l", xlab = 'Day', ylab = colnames(daily_data)[i], col=colors[j], main =paste(as.character(colnames(daily_data)[i]),"measurement") )
  }
  else{
    plot(daily_data$Date, daily_data[,i],"l", xlab = 'Day', ylab = colnames(daily_data)[i], col=colors[j], main =paste(as.character(colnames(daily_data)[i]),"concentration") )
  }
  
  
}
```

The variables starting with PT are the responses of the sensors measuring the concentration of the gas concerned. When investigating our data and by doing some research on air pollution, we found that the main air pollutants belong to the nitrogen oxides family (NOx). Thus, we wanted to see the relation between this gas concentration and the other ones. A linear regression was run between the NOx concentration and the other gases. 

```{r, echo=FALSE}
reg<-lm(daily_data$PT08.S3.NOx.~+daily_data$PT08.S1.CO.+daily_data$PT08.S2.NMHC.+daily_data$PT08.S4.NO2.+daily_data$PT08.S5.O3.+daily_data$T+daily_data$RH+daily_data$AH)
```


```{r}
summary(reg)
```

We will build the NOx concentration as a time series beacause it comes from sensor measurements, thus noise will be modeled accordingly. Since it is linearly related to the other gases (except for the temperature, relative humidity and absolute humidity that we will exclude from our study). We will focus on the NOx model. 

\subsection{2) Data preprocessing}

We count one PT08.S3.NOX observation per day for 341 days. We suppose that our time series follows an additive model :
$$
D_t = S_t + T_t +  X_t 
$$
Where $(S_t)_t$ is the seasonality, $(T_t)_t$ the trend and $(X_t)_t$  is assumed to be stationary.


We obtain this representation : 


```{r, echo=FALSE}
ts_PT08.S3.NOx. = ts(daily_data$PT08.S3.NOx., start=1, frequency = 1)
plot(ts_PT08.S3.NOx., col="violetred2", ylab="Daily NOx Concentration" )
```

Before building any model, we have to stationarise it first by removing the seasonal and trend components.

$$
X_t = D_t - T_t - S_t
$$

So we use differencing : 

$$
D_t - D_{t-1}
$$

```{r, echo=FALSE}
ts_stat_PT08.S3.NOx. = diff(ts_PT08.S3.NOx.)
plot(ts_stat_PT08.S3.NOx., col='blue', ylab='')
```

We want to make sure of the stationarity of our series, so we use the Augmented Dickey-Fuller Test that tests the null hypothesis that a unit root is present in a time series sample.

We obtain these results : 

```{r, echo=FALSE}
library(tseries)
test_stationnarity = adf.test(ts_stat_PT08.S3.NOx.)
print(test_stationnarity)
```

Since our p-value is smalled than 0.01, we reject the null hypothesis with level of confidence of 99%

\section{II)-Model fitting on the time series of interest}

In order to test different models, we take out the last 10 most recent data that will be used for testing, the other observations will be used for the training. 

```{r, echo=FALSE}
ts_stat_PT08.S3.NOx._train_set = ts_stat_PT08.S3.NOx.[1:(length(ts_stat_PT08.S3.NOx.)-10 )]
```

Let $(X_t)_t$ be a centered second order stationary process. For $h \in Z$ , we define : 

* The autocovariance function : 

$$
\gamma_{_X}(h) = Cov(X_t, X_{t+h}) = Cov(X_0, X_h) = E[X_0X_h]
$$

* The autorrelation function (ACF): 

$$
\rho_{_X}(h) = \rho(X_t, X_{t+h}) = \frac{\gamma_{_X}(h)}{\gamma_{_X}(0)}
$$

* The partial autocorrelation function (PACF):

$$
\tilde{\rho}_{_X}(h) = \rho_{_X}(X_0 - \pi_{h-1}(X_0), X_h - \pi_{h-1}(X_h))
$$
with the convention $\pi_0(X_1) = 0$ where $\pi_{h-1}(X_0)$ is the projection of $X_0$ on the linear span of $(X_1,X_2,....,X_{h-1})$ 

\subsection{1) MA model}

A MA(q) process, with $q \in N$, is a solution to the equation : 
$$
X_t = Z_t + \gamma_1Z_{t-1} + ... + \gamma_qZ_{t-q}
$$
with $t \in Z$ and $(Z_{t})_t$ a white noise. 

In order to find q, we use this MA(q) property : 
$$
\gamma_{_X}(h) = 0 \ \forall h \ge p
$$

We choose the q parameter accordingly to the last lag in the acf that is significantly non-null, outside the blue confident band.

```{r, echo=FALSE}
acf(ts_stat_PT08.S3.NOx._train_set, main = "ACF")
```

We notice that we can not fit our data into a MA model, which is quite unexpected, because the data comes from sensor measurements so we have expected a strong noise presence. Thus, we will try other models.

\subsection{2) AR model}

A second model is the AR(p).

An AR(p) process, with $p \in N$, is a solution of the equation : 

$$
X_t = \phi_{1}X_{t-1} + \phi_{2}X_{t-2} + ... + \phi_{p}X_{t-p} + Z_{t} 
$$

We will use a pacf AR(p) proprety, equivalent to the acf MA(q) property : 

$$
\tilde{\rho}_{_X}(h) = 0 \ \forall h>p 
$$
We obtain this pacf plot : 

```{r, echo=FALSE}
pacf(ts_stat_PT08.S3.NOx._train_set, main = "PACF")
```

The PACF plot indicates a significant value at lag 5. Thus, we choose an AR(5) model.

```{r, echo=FALSE}
fit_ar_model = arima(ts_stat_PT08.S3.NOx._train_set,order=c(5,0,0), include.mean = FALSE)
fit_ar_model
```

So we have : 

$$
X_t = -0.2842X_{t-1}-0.3523X_{t-2}-0.3167X_{t-3}-0.2801X_{t-4}-0.1998X_{t-5}
$$

Now we check that the residuals are likely white noise.

```{r, echo=FALSE}
tsdiag(fit_ar_model)
```

The ACF plot of residuals show no significant lags, so the AR(5) is likely a good representation of the series. 
Also, the p-values for Ljung-Box statistic are all greater than 0.05, so we cannot reject the
hypothesis that the autocorrelation is different from 0. Therefore, the AR(5) model is an
appropriate one.

\subsection{3) ARMA model}

\subsection{4) Residuals} 

\subsection{5) GARCH model}

\subsection{6) Prediction intervals for the 10 most recent data}

\section{III)-Training on the times series of interest using explanatory times series}

Yet, we will introduce other components, that we didn'tuse in the models previously. We do it in order to find better predictions and better confidence intervals.

\subsection{1) Preprocessing}

First, we have to stationarise our data with the same method as before. This means that we remove seasonal and trend components by using differencing.
We do it for each component with a name starting with PT.

To be sure that everything is fine, we use the Augmented Dickey-Fuller Test.

\subsection{2) Time varying coefficients}

Now we want to build a dynamical model thanks to the explanatory time series. We know that the order of the AR model was 5, so we will use 5 past values of the time series of interest for predicting the present value of the time series of interest too.

We use the SSModel function :

\subsection{3) QLIK}

\subsection{4) Prediction}

\section{IV) Conclusion}