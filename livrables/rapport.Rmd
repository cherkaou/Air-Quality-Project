---
title: "Air quality project : Time Series Analysis"
author : Yousra Cherkaoui Tangi - Lou Peltier - Guilhem Bois
output: pdf_document
date : "`r format(Sys.time(), '%d %B, %Y')`"
---

\tableofcontents
\newpage
\section{Preliminary }

Our project consists of time series analysis using R programming language. Our data set contains the responses of a gas mutisensor device deployed on the field in an Italian city, in a significantly polluted area, at road level. The data set contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. Data were recorded from March 2004 to February 2005. 


```{r, echo=FALSE}
#Spécifier le chemin du fichier .csv 
#fic = "/home/yousra/3A/cours/SeriesTemp/Air-Quality-Project/data/AirQualityUCI.csv"
fic = "/home/yousra/3A/cours/SeriesTemp/Air-Quality-Project/data/AirQualityUCI.csv"
data = read.csv(fic, header = TRUE, sep = ";", stringsAsFactors = FALSE)
```


```{r}
dim(data)
str(data)
```
| Attribute 	    	|Description                                                                                           |
|:-----------------:|:---------------------------------------------------------------------------------------------------- |    
| Date 		          | Date (DD/MM/YYYY)                                                                                    |
| Time 		          | Time (HH.MM.SS)                                                                                      |
| CO(GT) 		        | True hourly averaged concentration CO in mg/m^3 (reference analyzer)                                 |
| PT08.S1(CO)     	| PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)                          |
| NMHC(GT) 		      | True hourly averaged overall Non Metanic Hydro Carbons concentration in microg/m^3(reference analyzer |
| C6H6(GT) 		      | True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)                        |
| PT08.S2(NMHC)     | PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)                          |
| NOx(GT) 		      | True hourly averaged NOx concentration in ppb (reference analyzer)                                   |
| PT08.S3(NOx)    	| PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)                    |
| NO2(GT) 		      | True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)                            |
| PT08.S4(NO2)    	| PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)                    |
| PT08.S5(O3)     	| PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)                       |
| T 		            | Temperature in °C                                                                                   |
| RH 		            | Relative Humidity (%)                                                                                |
| AH 		            | AH Absolute Humidity                                                                                 |
 		

\section{I)-Cleaning and preprocessing}
\subsection{1) Data cleaning}
As we can see in the str(dim) output, our data contains missing values, some are directly reported as NA and others have been assigned to -200. It also contains a certain number of numeric variables that are characters which makes it not directly exploitable and some numbers have commas within them. For all these reasons, we had to clean our data set first so that we can exploit it afterwards.

```{r, echo=FALSE}
library(Amelia)
library(data.table)
missmap(data, main = "Missing values vs observed")
```


We remove X1 and X beacause they have no data within them, also we compute the necessary changes so that our data is exploitable afterwards.

```{r, echo=FALSE}
data$X = NULL
data$X.1 = NULL
#On supprime les lignes contenant des valeurs manquantes 
for(i in 1:15){
  data = subset(data, !is.na(data[,i]))
}

data$Date = as.Date(data$Date,format='%d/%m/%Y' )
data$CO.GT. = as.numeric(sub(",", ".", data$CO.GT.))
data$C6H6.GT. = as.numeric(sub(",", ".", data$C6H6.GT.))
data$T = as.numeric(sub(",", ".", data$T))
data$RH = as.numeric(sub(",", ".", data$RH))
data$AH = as.numeric(sub(",", ".", data$AH))
```

We noticed that NHMC contains a lot of -200 values, so we prefered to delet it along with all the lines that contain -200. 
```{r}
summary(data$NMHC.GT.)
```

```{r, echo=FALSE}
data$NMHC.GT.=NULL
for(j in 1:14){
 data = subset(data, data[,j]!=-200)
}

```

```{r}
str(data)
```

Now that our data is nearly clean, we chose to consider the daily concentration of the different gases our main focus of study by taking the average of the concentrations observed daily. 
```{r, echo=FALSE}
daily_data = copy(data)
daily_data$Time = NULL
daily_data = apply(daily_data[,2:13],2,tapply, daily_data$Date,mean )
daily_data = as.data.frame(daily_data)
setDT(daily_data, keep.rownames = "Date")
setDF(daily_data)
daily_data$Date = as.Date(daily_data$Date,format="%Y-%m-%d" )
```

We obtain 341 observations of 13 variables.

```{r}
str(daily_data)
```
```{r}
missmap(daily_data, main = "Missing values vs observed")
```

Since our data is all observed now, we can start plotting it to visualize what our data set looks like. 


```{r, echo=FALSE}
par(mfrow=c(2,2))
colors = c('red', 'blue', 'green','violet','orange','yellow','purple','brown')
v=c(11,12,13)
for(i in 2:13){
  j = sample(1:length(colors),1)
  if(i %in% v){
    plot(daily_data$Date, daily_data[,i],"l", xlab = 'Day', ylab = colnames(daily_data)[i], col=colors[j], main =paste(as.character(colnames(daily_data)[i]),"measurement") )
  }
  else{
    plot(daily_data$Date, daily_data[,i],"l", xlab = 'Day', ylab = colnames(daily_data)[i], col=colors[j], main =paste(as.character(colnames(daily_data)[i]),"concentration") )
  }
  
  
}
```

The variables starting with PT are the responses of the sensors measuring the concentration of the gas concerned. When investigating our data and by doing some research on air pollution, we found that the main air pollutants belong to the nitrogen oxides family (NOx). Thus, we wanted to see the relation between this gas concentration and the other ones. A linear regression was run between the NOx concentration and the other gases. 

```{r, echo=FALSE}
reg<-lm(daily_data$PT08.S3.NOx.~+daily_data$PT08.S1.CO.+daily_data$PT08.S2.NMHC.+daily_data$PT08.S4.NO2.+daily_data$PT08.S5.O3.+daily_data$T+daily_data$RH+daily_data$AH)
```


```{r}
summary(reg)
```

We will build the NOx concentration as a time series beacause it comes from sensor measurements, thus noise will be modeled accordingly. Since it is linearly related to the other gases (except for the temperature, relative humidity and absolute humidity that we will exclude from our study). We will focus on the NOx model. 

\subsection{2) Data preprocessing}

We count one PT08.S3.NOX observation per day for 341 days. We suppose that our time series follows an additive model :
$$
D_t = S_t + T_t +  X_t 
$$
Where $(S_t)_t$ is the seasonality, $(T_t)_t$ the trend and $(X_t)_t$  is assumed to be stationary.


We obtain this representation : 


```{r, echo=FALSE}
ts_PT08.S3.NOx. = ts(daily_data$PT08.S3.NOx., start=1, frequency = 1)
plot(ts_PT08.S3.NOx., col="violetred2", ylab="Daily NOx Concentration" )
```

Before building any model, we have to stationarise it first by removing the seasonal and trend components.

$$
X_t = D_t - T_t - S_t
$$

So we use differencing : 

$$
D_t - D_{t-1}
$$

```{r, echo=FALSE}
ts_stat_PT08.S3.NOx. = diff(ts_PT08.S3.NOx.)
plot(ts_stat_PT08.S3.NOx., col='blue', ylab='')
```

We want to make sure of the stationarity of our series, so we use the Augmented Dickey-Fuller Test that tests the null hypothesis that a unit root is present in a time series sample.

We obtain these results : 

```{r, echo=FALSE}
library(tseries)
test_stationnarity = adf.test(ts_stat_PT08.S3.NOx.)
print(test_stationnarity)
```

Since our p-value is smalled than 0.01, we reject the null hypothesis with level of confidence of 99%

\section{II)-Model fitting on the time series of interest}

In order to test different models, we take out the last 10 most recent data that will be used for testing, the other observations will be used for the training. 

```{r, echo=FALSE}
ts_stat_PT08.S3.NOx._train_set = ts_stat_PT08.S3.NOx.[1:(length(ts_stat_PT08.S3.NOx.)-10 )]
```

Let $(X_t)_t$ be a centered second order stationary process. For $h \in Z$ , we define : 

* The autocovariance function : 

$$
\gamma_{_X}(h) = Cov(X_t, X_{t+h}) = Cov(X_0, X_h) = E[X_0X_h]
$$

* The autorrelation function (ACF): 

$$
\rho_{_X}(h) = \rho(X_t, X_{t+h}) = \frac{\gamma_{_X}(h)}{\gamma_{_X}(0)}
$$

* The partial autocorrelation function (PACF):

$$
\tilde{\rho}_{_X}(h) = \rho_{_X}(X_0 - \pi_{h-1}(X_0), X_h - \pi_{h-1}(X_h))
$$
with the convention $\pi_0(X_1) = 0$ where $\pi_{h-1}(X_0)$ is the projection of $X_0$ on the linear span of $(X_1,X_2,....,X_{h-1})$ 

\subsection{1) MA model}

A MA(q) process, with $q \in N$, is a solution to the equation : 
$$
X_t = Z_t + \gamma_1Z_{t-1} + ... + \gamma_qZ_{t-q}
$$
with $t \in Z$ and $(Z_{t})_t$ a white noise. 

In order to find q, we use this MA(q) property : 
$$
\gamma_{_X}(h) = 0 \ \forall h \ge p
$$

We choose the q parameter accordingly to the last lag in the acf that is significantly non-null, outside the blue confident band.

```{r, echo=FALSE}
acf(ts_stat_PT08.S3.NOx._train_set, main = "ACF")
```

We notice that we can not fit our data into a MA model, which is quite unexpected, because the data comes from sensor measurements so we have expected a strong noise presence. Thus, we will try other models.

\subsection{2) AR model}

A second model is the AR(p).

An AR(p) process, with $p \in N$, is a solution of the equation : 

$$
X_t = \phi_{1}X_{t-1} + \phi_{2}X_{t-2} + ... + \phi_{p}X_{t-p} + Z_{t} 
$$

We will use a pacf AR(p) proprety, equivalent to the acf MA(q) property : 

$$
\tilde{\rho}_{_X}(h) = 0 \ \forall h>p 
$$
We obtain this pacf plot : 

```{r, echo=FALSE}
pacf(ts_stat_PT08.S3.NOx._train_set, main = "PACF")
```

The PACF plot indicates a significant value at lag 5. Thus, we choose an AR(5) model.

```{r, echo=FALSE}
fit_ar_model = arima(ts_stat_PT08.S3.NOx._train_set,order=c(5,0,0), include.mean = FALSE)
fit_ar_model
```

So we have : 

$$
X_t = -0.2842X_{t-1}-0.3523X_{t-2}-0.3167X_{t-3}-0.2801X_{t-4}-0.1998X_{t-5}
$$

Now we check that the residuals are likely white noise.

```{r, echo=FALSE}
tsdiag(fit_ar_model)
```

The ACF plot of residuals show no significant lags, so the AR(5) is likely a good representation of the series. 
Also, the p-values for Ljung-Box statistic are all greater than 0.05, so we cannot reject the
hypothesis that the autocorrelation is different from 0. Therefore, the AR(5) model is an
appropriate one.

\subsection{3) ARMA model}

\subsection{4) Residuals} 

\subsection{5) GARCH model}

\subsection{6) Prediction intervals for the 10 most recent data}

\section{III)-Training on the times series of interest using explanatory times series}

Yet, we will introduce other components, that we didn'tuse in the models previously. We do it in order to find better predictions and better confidence intervals.

\subsection{1) Preprocessing}

First, we have to stationarise our data with the same method as before. This means that we remove seasonal and trend components by using differencing.

```{r,echo=FALSE}
library(data.table)
#On choisit le fichier qui contient les données

path = file.choose()
source(path)
```


```{r,echo=FALSE}
library(KFAS)
library(ggplot2)
ggplot(daily_data,  aes(x=Date, y=PT08.S4.NO2.)) + geom_line()

```

We see indeed that this data need to be stationarised

```{r, echo=FALSE}
#we stationarise the dataset
ts_PT08.S4.NO2. = ts(daily_data$PT08.S4.NO2., start=1, frequency =1)
ts_stat_PT08.S4.NO2. = diff(ts_PT08.S4.NO2.)
daily_data$PT08.S4.NO2._stat <- c(0,diff(daily_data$PT08.S4.NO2.))
ggplot(data=daily_data, aes(x=Date, y=PT08.S4.NO2._stat)) + geom_line()
```

As before, we apply the Augmented Dickey-Fuller Test

```{r}

library(tseries)
test_stationnarity = adf.test(ts_stat_PT08.S4.NO2.)
print(test_stationnarity)
```

We do it for each component with a name starting with PT.

To be sure that everything is fine, we use the Augmented Dickey-Fuller Test.

Here are the results for each component.

```{r, echo=FALSE}
ggplot(daily_data,  aes(x=Date, y=PT08.S1.CO.)) + geom_line()
```

```{r, echo=FALSE}
#We stationarise the serie
ts_PT08.S1.CO. = ts(daily_data$PT08.S1.CO., start=1, frequency =1)
ts_stat_PT08.S1.CO. = diff(ts_PT08.S1.CO.)
daily_data$PT08.S1.CO._stat <- c(0,diff(daily_data$PT08.S1.CO.))
ggplot(data=daily_data, aes(x=Date, y=PT08.S1.CO._stat)) + geom_line()
```


```{r}
library(tseries)
test_stationnarity = adf.test(ts_stat_PT08.S1.CO.)
print(test_stationnarity)
```

```{r,echo=FALSE}
#We stationarise the serie
ts_PT08.S2.NMHC. = ts(daily_data$PT08.S2.NMHC., start=1, frequency =1)
ts_stat_PT08.S2.NMHC. = diff(ts_PT08.S2.NMHC.)
daily_data$PT08.S2.NMHC._stat <- c(0,diff(daily_data$PT08.S2.NMHC.))
ggplot(data=daily_data, aes(x=Date, y=PT08.S4.NO2._stat)) + geom_line()

```

```{r}
#
library(tseries)
test_stationnarity = adf.test(ts_stat_PT08.S2.NMHC.)
print(test_stationnarity)
```

```{r, echo=FALSE}
#We stationarise the serie
ts_PT08.S5.O3. = ts(daily_data$PT08.S5.O3., start=1, frequency =1)
ts_stat_PT08.S5.O3. = diff(ts_PT08.S5.O3.)
daily_data$PT08.S5.O3._stat <- c(0,diff(daily_data$PT08.S5.O3.))
ggplot(data=daily_data, aes(x=Date, y=PT08.S5.O3._stat)) + geom_line()
```

```{r}
#
library(tseries)
test_stationnarity = adf.test(ts_stat_PT08.S5.O3.)
print(test_stationnarity)
```

\subsection{2) Time varying coefficients}

Now we want to build a dynamical model thanks to the explanatory time series. We know that the order of the AR model was 5, so we will use 5 past values of the time series of interest for predicting the present value of the time series of interest too.

```{r, echo=FALSE}
#now that we have stationarized, we can create the model
ts_PTO8.S3.NOx. = ts(daily_data$PT08.S3.NOx., start=1, frequency = 1)
ts_stat_PTO8.S3.NOx. = diff(ts_PTO8.S3.NOx.)
n<-length(ts_stat_PTO8.S3.NOx.)
y<-ts(ts_stat_PTO8.S3.NOx. ,frequency = 4, start = c(1959,2))
ytraining<-ts(y[-((n-5):n)],frequency = 4, start = c(1959,2))

#FIRST <- diff(daily_data$PT08.S5.O3.)
#SECOND <- diff(daily_data$PT08.S1.CO.)
#THIRD <- diff(daily_data$PT08.S2.NMHC.)
#FOURTH <- diff(daily_data$PT08.S4.NO2.)
ts_PT08.S5.O3. = ts(daily_data$PT08.S5.O3., start=1, frequency = 1)
ts_stat_PT08.S5.O3. = diff(ts_PT08.S5.O3.)
FIRST = ts_stat_PT08.S5.O3.[(length(ts_stat_PT08.S5.O3.)-4 ):length(ts_stat_PT08.S5.O3.)]

ts_PT08.S1.CO. = ts(daily_data$PT08.S1.CO., start=1, frequency = 1)
ts_stat_PT08.S1.CO. = diff(ts_PT08.S1.CO.)
SECOND = ts_stat_PT08.S1.CO.[(length(ts_stat_PT08.S1.CO.)-4 ):length(ts_stat_PT08.S1.CO.)]

ts_PT08.S2.NMHC. = ts(daily_data$PT08.S2.NMHC., start=1, frequency = 1)
ts_stat_PT08.S2.NMHC. = diff(ts_PT08.S2.NMHC.)
THIRD = ts_stat_PT08.S2.NMHC.[(length(ts_stat_PT08.S2.NMHC.)-4 ):length(ts_stat_PT08.S2.NMHC.)]

ts_PT08.S4.NO2. = ts(daily_data$PT08.S4.NO2., start=1, frequency = 1)
ts_stat_PT08.S4.NO2. = diff(ts_PT08.S4.NO2.)
FOURTH = ts_stat_PT08.S4.NO2.[(length(ts_stat_PT08.S4.NO2.)-4 ):length(ts_stat_PT08.S4.NO2.)]

Y<-ts(cbind(ytraining,lag(ytraining,1),lag(ytraining,2),lag(ytraining,3),lag(ytraining,4),lag(ytraining,5),lag(ytraining,6),lag(ytraining,7),lag(ytraining,8),lag(ytraining,9),lag(ytraining,10)))
Y <- Y[-c(1:5),]
tail(Y)   # NA for the lagged versions at the end because we do not know the future!
```

We use the SSModel function. Our target variable is ts_PTO8.S3.NOx. and our covariates are the other components starting with PT.


\subsection{3) QLIK}

We have our model so we can use it with a QFAS in order to tune the hyperparameter.

\subsection{4) Prediction}

We have everything yet in order to do the prediction. The intervals of prediction can indeed be produced thanks to the use of the Kalman’s recursion on the tuned dynamical model.

Let's explain why we use this : by contrast with the AR models, it is much more difficult to find the best possible (linear) prediction of an ARMA mode. Indeed, as soon as the MA part is non degenerate, the filter can have infinitely many non null coefficients.

One way to solve the problem is to consider ARMA model as a more general linear model called state space models. Those models have been introduced in signal processing and the best linear prediction can be computed recursively by the Kalman’s recursion.
How does this work ?
A state space linear model of dimension r with constant coefficient is given by a system of space euqation and state equations of the form : $X_t = G^TY_t +Z_T$
                         $Y_t = FY_{t-1} +V_t$
which are respectively the Space equation and the State equation, where $(Z_t)$ and $(V_t)$ are uncorrelated white noise with variance R and Q, G $\in$ Rr, F $\in$ M(r,r) and Y $\in$ Rr is the random state of the system. The Kalman theorem says : In a state-space model with constant coefficients, if  

$\widehat{Y_0}$ and $\Omega_0$ are well chosen, one can compute recursively 

$$ 
\widehat{X_n} = \pi_{n-1}(X_n)
$$
$$\\ R_n^L = E[(X_n - \widehat{X_n})^2] $$
$$\\ \widehat{Y_n} = \pi_{n-1}(Y_n) $$
$$\\and\ \Omega_n = E[(Y_n - \widehat{Y_n})(Y_n - \widehat{Y_n})^T] $$
$$\\by\ the\ following\ recursion\ : \\
\widehat{Y_{n+1}} = F\widehat{Y_n} + \frac{F\Omega_nG}{R_n^L}*(X_n -G^T\widehat{Y_n}) \\$$
$$\widehat{X_{n+1}} = (G)^T*\widehat{Y_{n+1}} \\$$
$$\Omega_{n+1} = F\Omega_nF^T +Q- \frac{F\Omega_nG}{R_n}*G^T\Omega_nF^T \\$$
$$
R_{n+1}^L = G^T\Omega_{n+1}G+R 
$$

The Kalman’s recursion has several advantages :
- It is a recursve procedures
- Each step requires the inversion of a scalar RL n and not the entire covariance matrix
- The recursion can handle missing values nicely

However, there are also some drawbacks :
- Tuning hyperparameters requires a non explicit minimization
- The recursion can be instable

Our target variable is ts_PTO8.S3.NOx. and our covariates are the other components starting with PT.

\section{IV) Conclusion}
In conclusion, this project allowed us to implement various time series models on R, namely AR, MA, ARMA and GARCH and also to get an insight on the Kalman recursion. It also allowed us to learn about versioning tools such as Github in order to easilty share our code among us and finally, to practice latex and markdown. 